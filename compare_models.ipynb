{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOb0qFEydd1k+g3gRHUzvW+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7iCkSu8Czs2Z"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","source":["!pip install -U segmentation-models-pytorch albumentations pytorch-lightning rich"],"metadata":{"id":"xPAnY5zz5qD2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import cv2\n","import torch\n","import matplotlib.pyplot as plt\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar, RichProgressBar, LearningRateMonitor\n","from pytorch_lightning.loggers import TensorBoardLogger\n","import segmentation_models_pytorch as smp\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset as BaseDataset\n","import json\n","import albumentations as albu\n","from timeit import default_timer as timer\n","from typing import Any\n","\n","# Ignore annoying certificate problem\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","torch.hub.set_dir(\"/content/drive/MyDrive/Colab Notebooks/weights\")\n","DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/lpr_seg/train_set/'\n","\n","%load_ext tensorboard"],"metadata":{"id":"luVGz9XN0O5g","executionInfo":{"status":"ok","timestamp":1669951327742,"user_tz":-360,"elapsed":7594,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"fnfcNV980PMf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669951351148,"user_tz":-360,"elapsed":23421,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}},"outputId":"a22a9fdf-b4c3-446e-fcf3-37e348abe420"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# helper function for data visualization\n","def visualize(**images):\n","    \"\"\"PLot images in one row.\"\"\"\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()\n","\n","\n","class DefectsDataset(BaseDataset):\n","    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n","    \n","    Args:\n","        images_dir (str): path to images folder\n","        masks_dir (str): path to segmentation masks folder\n","        class_values (list): values of classes to extract from segmentation mask\n","        augmentation (albumentations.Compose): data transfromation pipeline \n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing \n","            (e.g. noralization, shape manipulation, etc.)\n","    \n","    \"\"\"\n","    \n","    CLASSES = ['background', 'plates']\n","    \n","    def __init__(\n","            self, \n","            root,\n","            classes=None, \n","            augmentation=None, \n","            preprocessing=None,\n","            new_shape=None,\n","    ):\n","        self.root = root\n","        images_dir = os.path.join(self.root, \"images\")\n","        self.ids = os.listdir(images_dir)\n","        self.ids.sort()\n","        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n","\n","        masks_dir = os.path.join(self.root, \"masks\")\n","        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n","\n","        # convert str names to class values on masks\n","        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n","        \n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","        self.new_shape = new_shape\n","    \n","    def __getitem__(self, i):\n","        \n","        # read image and mask\n","        file_name = self.ids[i]\n","        # image = cv2.imread(self.images_fps[i], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n","        image = cv2.imread(self.images_fps[i])\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        mask = cv2.imread(self.masks_fps[i], cv2.IMREAD_GRAYSCALE)\n","\n","        # resize image and mask\n","        # width and height must be divisible by 32\n","        if self.new_shape is None:\n","            new_width = 1024\n","            factor = new_width / image.shape[1]\n","            new_height = factor * image.shape[0]\n","            new_height = int(np.ceil(new_height / 32) * 32)\n","            new_shape = (new_width, new_height)\n","        else:\n","            new_shape = self.new_shape\n","        image = cv2.resize(image, new_shape)\n","        mask = cv2.resize(mask, new_shape)\n","        \n","        # extract certain classes from mask (e.g. cars)\n","        masks = [(mask == v) for v in self.class_values]\n","        mask = np.stack(masks, axis=-1)\n","        for i in range(1, mask.shape[-1]):\n","            mask[:, :, i - 1] = np.bitwise_or(mask[:, :, i - 1], mask[:, :, i])\n","        \n","        # apply augmentations\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","        \n","        # apply preprocessing\n","        mask = mask.transpose(2, 0, 1).astype(np.float32)\n","        image = image.transpose(2, 0, 1).astype(np.float32)\n","        image /= 255.0\n","            \n","        return {'image': image, 'mask': mask}\n","        \n","    def __len__(self):\n","        return len(self.ids)"],"metadata":{"id":"GcP0YRC00gpB","executionInfo":{"status":"ok","timestamp":1669956974850,"user_tz":-360,"elapsed":10,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Lets look at data we have\n","\n","dataset = DefectsDataset(DATA_DIR, classes=['plates'])\n","print(f\"{len(dataset)} images in the dataset\")\n","\n","for i in range(5):\n","    input_dict = dataset[i]\n","    # print(input_dict)\n","    image, mask = input_dict['image'], input_dict['mask'] # get some sample\n","    print(image.shape, mask.shape)\n","    image = image.transpose(1, 2, 0)\n","    mask = mask.transpose(1, 2, 0)\n","    visualize(\n","        image=image, \n","        plates_mask=mask.squeeze(),\n","    )"],"metadata":{"id":"umz1xEaY0hy2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_training_augmentation():\n","    train_transform = [\n","\n","        # albu.HorizontalFlip(p=0.5),\n","\n","        # albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n","\n","        # albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n","        albu.OneOf(\n","            [\n","                albu.RandomCrop(height=480, width=480, p=0.1),\n","                albu.CropNonEmptyMaskIfExists(height=480, width=480, p=0.9)\n","            ],\n","            p=1.0,\n","        ),\n","        \n","\n","        # albu.GaussNoise(p=0.2),\n","        # albu.IAAPerspective(p=0.5),\n","\n","        # albu.OneOf(\n","        #     [\n","        #         albu.Sharpen(p=1),\n","        #         # albu.Blur(p=1),\n","        #         albu.MotionBlur(blur_limit=5, p=1),\n","        #         albu.Defocus(radius=4, p=1)\n","        #     ],\n","        #     p=0.1,\n","        # ),\n","\n","        # albu.OneOf(\n","        #     [\n","        #         # albu.CLAHE(p=1),\n","        #         albu.RandomGamma(p=1),\n","        #         albu.RandomBrightnessContrast(p=1),\n","        #         albu.HueSaturationValue(p=1),\n","        #     ],\n","        #     p=0.5,\n","        # ),\n","    ]\n","    return albu.Compose(train_transform)"],"metadata":{"id":"aw_f7_bZ0j6Y","executionInfo":{"status":"ok","timestamp":1669956983184,"user_tz":-360,"elapsed":7,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# #### Visualize resulted augmented images and masks\n","\n","# augmented_dataset = DefectsDataset(\n","#     DATA_DIR, \n","#     augmentation=get_training_augmentation(), \n","#     classes=['plates'],\n","# )\n","\n","# # same image with different random transforms\n","# for i in range(5):\n","#     input_dict = augmented_dataset[3]\n","#     image, mask = input_dict['image'], input_dict['mask'] # get some sample\n","#     print(image.shape, mask.shape)\n","#     image = image.transpose(1, 2, 0)\n","#     mask = mask.transpose(1, 2, 0)\n","#     visualize(\n","#         image=image, \n","#         plates_mask=mask.squeeze(),\n","#     )"],"metadata":{"id":"1Cj_hYHd0m9w","executionInfo":{"status":"ok","timestamp":1669956984846,"user_tz":-360,"elapsed":8,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["classes = ['plates']\n","# augment = True\n","augment = False\n","if augment:\n","    augmentations = get_training_augmentation()\n","else:\n","    augmentations = None\n","resize = False\n","if resize:\n","    new_shape = [960, 960]\n","else:\n","    new_shape = None\n","full_dataset = DefectsDataset(\n","    DATA_DIR, \n","    augmentation=augmentations, \n","    classes=classes,\n","    new_shape=new_shape\n",")\n","\n","full_loader = DataLoader(full_dataset, batch_size=1, shuffle=False, num_workers=2)"],"metadata":{"id":"hqKxkbP-0o7s","executionInfo":{"status":"ok","timestamp":1669956984848,"user_tz":-360,"elapsed":7,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["class DefectsModel(pl.LightningModule):\n","\n","    def __init__(self, arch, encoder_name, in_channels, out_classes, loss, lr=0.0001, **kwargs):\n","        super().__init__()\n","        self.model = smp.create_model(\n","            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n","        )\n","\n","        # preprocessing parameteres for image\n","        params = smp.encoders.get_preprocessing_params(encoder_name)\n","        self.register_buffer('std', torch.tensor(params['std']).view(1, 3, 1, 1))\n","        self.register_buffer('mean', torch.tensor(params['mean']).view(1, 3, 1, 1))\n","\n","        # for image segmentation dice loss could be the best first choice\n","        self.loss_mode = smp.losses.BINARY_MODE if out_classes == 1 else smp.losses.MULTILABEL_MODE\n","        self.metrics_mode = 'binary' if out_classes == 1 else 'multilabel'\n","        \n","        if loss == 'DiceLoss':\n","            self.loss_fn = smp.losses.DiceLoss(self.loss_mode, from_logits=True)\n","        elif loss == 'FocalLoss':\n","            self.loss_fn = smp.losses.FocalLoss(self.loss_mode)\n","        elif loss == 'JaccardLoss':\n","            self.loss_fn = smp.losses.JaccardLoss(self.loss_mode, from_logits=True)\n","        else:\n","            raise Exception(\"Unsupported loss\")\n","\n","        self.lr = lr\n","\n","    def forward(self, image):\n","        # normalize image here\n","        image = (image - self.mean) / self.std\n","        mask = self.model(image)\n","        return mask\n","\n","    def shared_step(self, batch, stage):\n","        image = batch['image']\n","\n","        # Shape of the image should be (batch_size, num_channels, height, width)\n","        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n","        assert image.ndim == 4\n","\n","        # Check that image dimensions are divisible by 32, \n","        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of \n","        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have \n","        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n","        # and we will get an error trying to concat these features\n","        h, w = image.shape[2:]\n","        assert h % 32 == 0 and w % 32 == 0\n","\n","        mask = batch['mask']\n","\n","        # Shape of the mask should be [batch_size, num_classes, height, width]\n","        # for binary segmentation num_classes = 1\n","        assert mask.ndim == 4\n","\n","        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n","        assert mask.max() <= 1.0 and mask.min() >= 0\n","\n","        logits_mask = self.forward(image)\n","        prob_mask = logits_mask.sigmoid()\n","        \n","        # Predicted mask contains logits, and loss_fn param `from_logits` is set\n","        loss = self.loss_fn(logits_mask, mask)\n","        self.log(f\"{stage}_loss\", loss)\n","\n","        # Lets compute metrics for some threshold\n","        # first convert mask values to probabilities, then \n","        # apply thresholding\n","        pred_mask = (prob_mask > 0.5).float()\n","\n","        # We will compute IoU metric by two ways\n","        #   1. dataset-wise\n","        #   2. image-wise\n","        # but for now we just compute true positive, false positive, false negative and\n","        # true negative 'pixels' for each image and class\n","        # these values will be aggregated in the end of an epoch\n","        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=self.metrics_mode)\n","\n","        return {\n","            \"loss\": loss,\n","            \"tp\": tp,\n","            \"fp\": fp,\n","            \"fn\": fn,\n","            \"tn\": tn,\n","        }\n","\n","    def shared_epoch_end(self, outputs, stage):\n","        # aggregate step metics\n","        tp = torch.cat([x[\"tp\"] for x in outputs])\n","        fp = torch.cat([x[\"fp\"] for x in outputs])\n","        fn = torch.cat([x[\"fn\"] for x in outputs])\n","        tn = torch.cat([x[\"tn\"] for x in outputs])\n","\n","        # per image IoU means that we first calculate IoU score for each image \n","        # and then compute mean over these scores\n","        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n","        \n","        # dataset IoU means that we aggregate intersection and union over whole dataset\n","        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n","        # in this particular case will not be much, however for dataset \n","        # with \"empty\" images (images without target class) a large gap could be observed. \n","        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n","        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\n","        metrics = {\n","            f\"{stage}_per_image_iou\": per_image_iou,\n","            f\"{stage}_dataset_iou\": dataset_iou,\n","        }\n","        \n","        self.log_dict(metrics, prog_bar=True)\n","\n","    def training_step(self, batch, batch_idx):\n","        return self.shared_step(batch, \"train\")            \n","\n","    def training_epoch_end(self, outputs):\n","        return self.shared_epoch_end(outputs, \"train\")\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.shared_step(batch, \"valid\")\n","\n","    def validation_epoch_end(self, outputs):\n","        return self.shared_epoch_end(outputs, \"valid\")\n","\n","    def test_step(self, batch, batch_idx):\n","        return self.shared_step(batch, \"test\")  \n","\n","    def test_epoch_end(self, outputs):\n","        return self.shared_epoch_end(outputs, \"test\")\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n","        # optimizer = torch.optim.SGD(self.parameters(), lr=self.lr, momentum=0.9)\n","        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n","        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=25)\n","        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'train_loss'}\n","        # return optimizer"],"metadata":{"id":"xAJKkiiq1Nuq","executionInfo":{"status":"ok","timestamp":1669956995198,"user_tz":-360,"elapsed":9,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["def decode_file_name(file_name):\n","    \"\"\"\n","    Decodes parameters from a checkpoint file_name.\n","    \"\"\"\n","    args = file_name.split('_')\n","    model_name = args[2]\n","    if args[3] == 'se':\n","        encoder_name = f\"{args[3]}_{args[4]}_{args[5]}\"\n","    else:\n","        encoder_name = args[3]\n","    if args[-3][0] == 'a':\n","        augment = True if args[-3][4] == 'T' else False\n","        out_classes = int(args[-4][-1])\n","    else:\n","        augment = False\n","        out_classes = int(args[-3][-1])\n","    loss = args[-2]\n","\n","    return model_name, encoder_name, augment, out_classes, loss\n","\n","file_name = \"lpr_lightning_Unet_se_resnext50_32x4d_c=1_aug=True_JaccardLoss_loss-0.9469.ckpt\"\n","model_name, encoder_name, augment, out_classes, loss = decode_file_name(file_name)\n","print(model_name, encoder_name, out_classes, augment, loss)\n","best_model = DefectsModel.load_from_checkpoint(\n","    f\"/content/drive/MyDrive/Colab Notebooks/weights/lightning/{file_name}\",\n","    arch=model_name, encoder_name=encoder_name, in_channels=3, out_classes=out_classes, loss=loss)"],"metadata":{"id":"XzvWiPHy1WcM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669957005251,"user_tz":-360,"elapsed":1692,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}},"outputId":"c6cbb685-79f3-46d9-fb99-9a7dabed5cd7"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Unet se_resnext50_32x4d 1 True JaccardLoss\n"]}]},{"cell_type":"code","source":["best_model = best_model.model\n","sigmoid_on = True"],"metadata":{"id":"p7XcYV1z1dFR","executionInfo":{"status":"ok","timestamp":1669957005254,"user_tz":-360,"elapsed":11,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# file_name = \"FPNmod_se_resnext50_32x4d_loss_0.04456.pth\"\n","# best_model = torch.load(f\"/content/drive/MyDrive/Colab Notebooks/weights/lightning/{file_name}\")\n","# # print(best_model)\n","# encoder_name = \"se_resnext50_32x4d\"\n","# out_classes = 1\n","# sigmoid_on = False"],"metadata":{"id":"SFPzMRCf1etW","executionInfo":{"status":"ok","timestamp":1669953073936,"user_tz":-360,"elapsed":15,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["DEVICE = 'cuda'\n","# DEVICE = 'cpu'\n","params = smp.encoders.get_preprocessing_params(encoder_name)\n","std = torch.tensor(params['std']).view(1, 3, 1, 1).to(DEVICE)\n","mean = torch.tensor(params['mean']).view(1, 3, 1, 1).to(DEVICE)\n","metrics_mode = 'binary' if out_classes == 1 else 'multilabel'\n","best_model.to(DEVICE)\n","\n","best_model.eval()\n","dataset_iou = 0.0\n","with torch.no_grad():\n","    for i in range(len(full_dataset)):\n","        if i % 100 == 0:\n","            print(f\"Image {i}\")\n","        input_dict = full_dataset[i]\n","        image, gt_mask = input_dict['image'], input_dict['mask']\n","\n","        gt_mask = torch.from_numpy(gt_mask).to(DEVICE).unsqueeze(0)\n","\n","        x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n","        x_tensor = (x_tensor - mean) / std\n","        if sigmoid_on:\n","            pr_mask = best_model(x_tensor).sigmoid()\n","        else:\n","            pr_mask = best_model(x_tensor)\n","        pr_mask = (pr_mask > 0.5).float()\n","        tp, fp, fn, tn = smp.metrics.get_stats(pr_mask.long(), gt_mask.long(), mode=metrics_mode)\n","        dataset_iou += smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","        if i % 100 == 0:\n","            print(f\"IOU: {dataset_iou / (i + 1)}\")\n","\n","print(f\"Total IOU: {dataset_iou / len(full_dataset)}\")"],"metadata":{"id":"fVu2_3rS1gm0","executionInfo":{"status":"error","timestamp":1669957325323,"user_tz":-360,"elapsed":3866,"user":{"displayName":"Alexey Konanykhin","userId":"14109563105925800583"}},"colab":{"base_uri":"https://localhost:8080/","height":417},"outputId":"886427e8-f839-4ec7-ceb4-312a8f6ac64e"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Image 2200\n","IOU: nan\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-bcb8b3cd582d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpr_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpr_mask\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mdataset_iou\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miou_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"IOU: {dataset_iou / (i + 1)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/segmentation_models_pytorch/metrics/functional.py\u001b[0m in \u001b[0;36miou_score\u001b[0;34m(tp, fp, fn, tn, reduction, class_weights, zero_division)\u001b[0m\n\u001b[1;32m    416\u001b[0m ) -> torch.Tensor:\n\u001b[1;32m    417\u001b[0m     \u001b[0;34m\"\"\"IoU score or Jaccard index\"\"\"\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     return _compute_metric(\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0m_iou_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/segmentation_models_pytorch/metrics/functional.py\u001b[0m in \u001b[0;36m_compute_metric\u001b[0;34m(metric_fn, tp, fp, fn, tn, reduction, class_weights, zero_division, **metric_kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclass_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# lpr_lightning_Unet_se_resnext50_32x4d_c=1_aug=True_JaccardLoss_iou-0.9468.ckpt        0.9180\n","# FPNmod_se_resnext50_32x4d_loss_0.04456.pth                                            0.8924\n","# lpr_lightning_Unet_se_resnext50_32x4d_c=1_aug=True_JaccardLoss_loss-0.9469-old.ckpt   0.9177\n","# "],"metadata":{"id":"eR-gscknfDHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_time = 0.0\n","with torch.no_grad():\n","  for i in range(min(10000, len(full_dataset))):\n","      if i % 100 == 0:\n","          print(f\"Image {i}\")\n","      input_dict = full_dataset[i]\n","      image, gt_mask = input_dict['image'], input_dict['mask']\n","\n","      gt_mask = gt_mask.squeeze()\n","\n","      x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n","      x_tensor = (x_tensor - mean) / std\n","      start = timer()\n","      pr_mask = best_model(x_tensor).sigmoid()\n","      pr_mask = pr_mask.squeeze().cpu().numpy()\n","      pr_mask = pr_mask > 0.5\n","      total_time += timer() - start\n","      if i % 100 == 0:\n","            print(f\"Time: {total_time / (i + 1)}\")\n","\n","print(f\"Total time: {total_time / len(full_dataset)}\")"],"metadata":{"id":"nvBKC-5qDJVZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lpr_lightning_Unet_se_resnext50_32x4d_c=1_aug=True_JaccardLoss_iou-0.9468.ckpt        xxx\n","# FPNmod_se_resnext50_32x4d_loss_0.04456.pth                                            xxx\n","# lpr_lightning_Unet_se_resnext50_32x4d_c=1_aug=True_JaccardLoss_loss-0.9469-old.ckpt   0.1157\n","# "],"metadata":{"id":"5FirCAk6fB5e"},"execution_count":null,"outputs":[]}]}